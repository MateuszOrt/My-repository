{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "287b832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "     ---------------------------------------- 0.0/316.9 MB ? eta -:--:--\n",
      "     -------------------------------------- 0.0/316.9 MB 991.0 kB/s eta 0:05:20\n",
      "     ---------------------------------------- 0.2/316.9 MB 2.0 MB/s eta 0:02:40\n",
      "     ---------------------------------------- 0.5/316.9 MB 3.2 MB/s eta 0:01:39\n",
      "     ---------------------------------------- 1.0/316.9 MB 5.4 MB/s eta 0:00:59\n",
      "     ---------------------------------------- 2.1/316.9 MB 8.9 MB/s eta 0:00:36\n",
      "     --------------------------------------- 3.5/316.9 MB 12.4 MB/s eta 0:00:26\n",
      "      -------------------------------------- 5.6/316.9 MB 17.0 MB/s eta 0:00:19\n",
      "     - ------------------------------------- 8.6/316.9 MB 24.0 MB/s eta 0:00:13\n",
      "     - ------------------------------------ 11.7/316.9 MB 54.4 MB/s eta 0:00:06\n",
      "     - ------------------------------------ 14.6/316.9 MB 72.6 MB/s eta 0:00:05\n",
      "     -- ----------------------------------- 18.0/316.9 MB 72.6 MB/s eta 0:00:05\n",
      "     -- ----------------------------------- 21.9/316.9 MB 72.6 MB/s eta 0:00:05\n",
      "     --- ---------------------------------- 25.8/316.9 MB 81.8 MB/s eta 0:00:04\n",
      "     --- ---------------------------------- 29.1/316.9 MB 72.6 MB/s eta 0:00:04\n",
      "     --- ---------------------------------- 32.4/316.9 MB 81.8 MB/s eta 0:00:04\n",
      "     ---- --------------------------------- 35.2/316.9 MB 72.6 MB/s eta 0:00:04\n",
      "     ---- --------------------------------- 38.7/316.9 MB 72.6 MB/s eta 0:00:04\n",
      "     ----- -------------------------------- 42.6/316.9 MB 73.1 MB/s eta 0:00:04\n",
      "     ----- -------------------------------- 46.4/316.9 MB 81.8 MB/s eta 0:00:04\n",
      "     ------ ------------------------------- 50.1/316.9 MB 73.1 MB/s eta 0:00:04\n",
      "     ------ ------------------------------- 53.7/316.9 MB 72.6 MB/s eta 0:00:04\n",
      "     ------ ------------------------------- 57.1/316.9 MB 81.8 MB/s eta 0:00:04\n",
      "     ------- ------------------------------ 60.0/316.9 MB 73.1 MB/s eta 0:00:04\n",
      "     ------- ------------------------------ 63.2/316.9 MB 72.6 MB/s eta 0:00:04\n",
      "     ------- ------------------------------ 66.7/316.9 MB 72.6 MB/s eta 0:00:04\n",
      "     -------- ----------------------------- 69.8/316.9 MB 73.1 MB/s eta 0:00:04\n",
      "     -------- ----------------------------- 73.5/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     --------- ---------------------------- 76.5/316.9 MB 72.6 MB/s eta 0:00:04\n",
      "     --------- ---------------------------- 80.0/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     ---------- --------------------------- 83.9/316.9 MB 72.6 MB/s eta 0:00:04\n",
      "     ---------- --------------------------- 87.7/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     ---------- --------------------------- 91.4/316.9 MB 73.1 MB/s eta 0:00:04\n",
      "     ----------- -------------------------- 95.0/316.9 MB 73.1 MB/s eta 0:00:04\n",
      "     ----------- -------------------------- 98.9/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     ----------- ------------------------- 102.7/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     ------------ ------------------------ 106.3/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     ------------ ------------------------ 110.1/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     ------------- ----------------------- 113.3/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     ------------- ----------------------- 116.9/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     ------------- ----------------------- 119.7/316.9 MB 72.6 MB/s eta 0:00:03\n",
      "     -------------- ---------------------- 123.7/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     -------------- ---------------------- 127.5/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     --------------- --------------------- 131.1/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     --------------- --------------------- 134.9/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     ---------------- -------------------- 138.5/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     ---------------- -------------------- 142.2/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     ----------------- ------------------- 145.8/316.9 MB 72.6 MB/s eta 0:00:03\n",
      "     ----------------- ------------------- 148.8/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     ----------------- ------------------- 152.5/316.9 MB 81.8 MB/s eta 0:00:03\n",
      "     ------------------ ------------------ 156.1/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     ------------------ ------------------ 159.4/316.9 MB 72.6 MB/s eta 0:00:03\n",
      "     ------------------ ------------------ 162.3/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     ------------------- ----------------- 165.9/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     ------------------- ----------------- 169.6/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     -------------------- ---------------- 172.4/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     -------------------- ---------------- 176.0/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     -------------------- ---------------- 179.7/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     --------------------- --------------- 183.2/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     --------------------- --------------- 186.0/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     ---------------------- -------------- 189.4/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     ---------------------- -------------- 193.1/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     ----------------------- ------------- 197.1/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     ----------------------- ------------- 200.0/316.9 MB 73.1 MB/s eta 0:00:02\n",
      "     ----------------------- ------------- 203.0/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     ------------------------ ------------ 206.5/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     ------------------------ ------------ 210.0/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     ------------------------ ------------ 213.0/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     ------------------------- ----------- 216.9/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     ------------------------- ----------- 220.8/316.9 MB 72.6 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 223.6/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 226.5/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 230.1/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 233.8/316.9 MB 81.8 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 237.4/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ---------------------------- -------- 241.0/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ---------------------------- -------- 244.8/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ------- 248.5/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ------- 252.3/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ------- 255.9/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------ ------ 259.7/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------ ------ 263.4/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------- ----- 266.8/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------- ----- 270.5/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 274.2/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 277.9/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 281.5/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 285.0/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 288.0/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 291.3/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 294.9/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 298.3/316.9 MB 73.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 302.1/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 306.0/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  309.0/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  312.6/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.5/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/316.9 MB 81.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- 316.9/316.9 MB 18.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "     ---------------------------------------- 0.0/200.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 200.5/200.5 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425388 sha256=9c885f3a0d5ea9cf5f70980158ec5a4d540ffa1bf87d8dea4903b62a573e5967\n",
      "  Stored in directory: c:\\users\\ortek\\appdata\\local\\pip\\cache\\wheels\\41\\4e\\10\\c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5471b88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/3d/c6/9bb3a165e915b9a43b2fd1d35620977bf1371e08538f3649585a1d7b4794/pandas-2.1.3-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading pandas-2.1.3-cp310-cp310-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\ortek\\anaconda3\\envs\\3.10\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ortek\\anaconda3\\envs\\3.10\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "     ---------------------------------------- 0.0/341.8 kB ? eta -:--:--\n",
      "     ---- ---------------------------------- 41.0/341.8 kB 1.9 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 163.8/341.8 kB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 341.8/341.8 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ortek\\anaconda3\\envs\\3.10\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.1.3-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/10.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/10.7 MB 10.2 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.9/10.7 MB 15.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.0/10.7 MB 23.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.8/10.7 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.4/10.7 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 43.5 MB/s eta 0:00:00\n",
      "Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "   ---------------------------------------- 0.0/502.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 502.5/502.5 kB ? eta 0:00:00\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.1.3 pytz-2023.3.post1 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4b51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5350afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"test.csv\",sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1ff5c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Krish</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sudhansh</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name  age\n",
       "0     Krish   31\n",
       "1  Sudhansh   30\n",
       "2     Sunny   29"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f1349c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "692fd86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Practise').getOrCreate() # Starting Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19c2a0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.30:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2978c1907c0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa9de11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('test.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "430a31db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b01c2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|     _c0|_c1|\n",
      "+--------+---+\n",
      "|    Name|age|\n",
      "|   Krish| 31|\n",
      "|Sudhansh| 30|\n",
      "|   Sunny| 29|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b09a861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: string]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.option('header','true').csv('test.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "985e83bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|age|\n",
      "+--------+---+\n",
      "|   Krish| 31|\n",
      "|Sudhansh| 30|\n",
      "|   Sunny| 29|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header','true').csv('test.csv',sep=';').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0fd335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv('test.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a02a12cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17c2d90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Krish', age='31'),\n",
       " Row(Name='Sudhansh', age='30'),\n",
       " Row(Name='Sunny', age='29')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aebe6c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema() # df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac1f96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv('test.csv',sep=';',inferSchema=True) # inferSchema sets data value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17ea54b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: int]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b2d7fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd71805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|age|\n",
      "+--------+---+\n",
      "|   Krish| 31|\n",
      "|Sudhansh| 30|\n",
      "|   Sunny| 29|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=spark.read.csv('test.csv',sep=';',header=True,inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a403de95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    Name|\n",
      "+--------+\n",
      "|   Krish|\n",
      "|Sudhansh|\n",
      "|   Sunny|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "358e580d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Name'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2dcc454c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('age', 'int')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "098fc406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Name: string, age: string]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb58acce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+\n",
      "|summary| Name| age|\n",
      "+-------+-----+----+\n",
      "|  count|    3|   3|\n",
      "|   mean| NULL|30.0|\n",
      "| stddev| NULL| 1.0|\n",
      "|    min|Krish|  29|\n",
      "|    max|Sunny|  31|\n",
      "+-------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "627463ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding columns\n",
    "df_pyspark=df_pyspark.withColumn('Experience',df_pyspark['age']-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ebde8a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Krish| 31|        11|\n",
      "|Sudhansh| 30|        10|\n",
      "|   Sunny| 29|         9|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f145710e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|age|\n",
      "+--------+---+\n",
      "|   Krish| 31|\n",
      "|Sudhansh| 30|\n",
      "|   Sunny| 29|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop a column\n",
    "df_pyspark.drop('Experience').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5571811c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|New Name|age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Krish| 31|        11|\n",
      "|Sudhansh| 30|        10|\n",
      "|   Sunny| 29|         9|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rename columns\n",
    "df_pyspark.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "daa4ba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Krish| 31|        11|\n",
      "|Sudhansh| 30|        10|\n",
      "|   Sunny| 29|         9|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show() # droppping nulls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1cabb84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Krish| 31|        11|\n",
      "|Sudhansh| 30|        10|\n",
      "|   Sunny| 29|         9|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "44315845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Krish| 31|        11|\n",
      "|Sudhansh| 30|        10|\n",
      "|   Sunny| 29|         9|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#treshold\n",
    "df_pyspark.na.drop(how='any',thresh=2).show() # If there are at least 2 non null values leave it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "942d061a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Krish| 31|        11|\n",
      "|Sudhansh| 30|        10|\n",
      "|   Sunny| 29|         9|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any',subset=['Name']).show() # Deletes all records that had null in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b8e2d0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Krish| 31|        11|\n",
      "|Sudhansh| 30|        10|\n",
      "|   Sunny| 29|         9|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing values','Experience').show()# puts 'Missing values' instead of null in table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ed5e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'Experience'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience']]\n",
    "    ).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "949da55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+-----------+------------------+\n",
      "|    Name|age|Experience|age_imputed|Experience_imputed|\n",
      "+--------+---+----------+-----------+------------------+\n",
      "|   Krish| 31|        11|         31|                11|\n",
      "|Sudhansh| 30|        10|         30|                10|\n",
      "|   Sunny| 29|         9|         29|                 9|\n",
      "+--------+---+----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa26e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|age|Experience|\n",
      "+--------+---+----------+\n",
      "|Sudhansh| 30|        10|\n",
      "|   Sunny| 29|         9|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filter operations\n",
    "# Age of people lees than or equal to 30\n",
    "# Add imputation cols to df\n",
    "df_pyspark.filter('age<=30').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "58860bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|age|Experience|\n",
      "+--------+---+----------+\n",
      "|Sudhansh| 30|        10|\n",
      "|   Sunny| 29|         9|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter('age<=30').select(['Name','age','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e0191bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|age|\n",
      "+--------+---+\n",
      "|Sudhansh| 30|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['age']<=30) & \n",
    "                  (df_pyspark['Experience']>=10)).select(['Name','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8cb522e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| Name|age|Experience|\n",
      "+-----+---+----------+\n",
      "|Krish| 31|        11|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(~(df_pyspark['age']<=30)).select(['Name','age','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b043198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark=SparkSession.builder.appName('Agg').getOrCreate() # Starting Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregatiom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8c06926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('test3.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ef06e788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4f6f3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b0b0eef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "082e0e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Groupby Departmernts  which gives maximum salary\n",
    "df_pyspark.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9c43f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Groupby Departmernts  which gives maximum salary\n",
    "df_pyspark.groupBy('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3e70a71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Groupby Departmernts  which gives maximum salary\n",
    "df_pyspark.groupBy('Departments').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3f6c5abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|max(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      20000|\n",
      "|    Sunny|      10000|\n",
      "|    Krish|      10000|\n",
      "|   Mahesh|       4000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Groupby Departmernts  which gives maximum salary\n",
    "df_pyspark.groupBy('Name').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c1e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examples of PySpark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "097168fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training=spark.read.csv('test1.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1acb1ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1631eb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c9c80b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5742b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Age,Experience]---->new feature----> independent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7ff1cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=[\"age\",\"Experience\"],outputCol=\"Independent Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3504a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=featureassembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "75a85f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+--------------------+\n",
      "|     Name|age|Experience|Salary|Independent Features|\n",
      "+---------+---+----------+------+--------------------+\n",
      "|    Krish| 31|        10| 30000|         [31.0,10.0]|\n",
      "|Sudhanshu| 30|         8| 25000|          [30.0,8.0]|\n",
      "|    Sunny| 29|         4| 20000|          [29.0,4.0]|\n",
      "|     Paul| 24|         3| 20000|          [24.0,3.0]|\n",
      "|   Harsha| 21|         1| 15000|          [21.0,1.0]|\n",
      "|  Shubham| 23|         2| 18000|          [23.0,2.0]|\n",
      "+---------+---+----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "12b11b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary', 'Independent Features']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fa199b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data=output.select(\"Independent Features\",\"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f1721ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Independent Features|Salary|\n",
      "+--------------------+------+\n",
      "|         [31.0,10.0]| 30000|\n",
      "|          [30.0,8.0]| 25000|\n",
      "|          [29.0,4.0]| 20000|\n",
      "|          [24.0,3.0]| 20000|\n",
      "|          [21.0,1.0]| 15000|\n",
      "|          [23.0,2.0]| 18000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "140e7f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "##train test split\n",
    "train_data,test_data=finalized_data.randomSplit([0.75,0.25])\n",
    "regressor=LinearRegression(featuresCol='Independent Features', labelCol='Salary')\n",
    "regressor=regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b19d191e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-64.8464, 1584.7554])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Coefficients\n",
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d478f00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15414.10693970376"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Intercepts\n",
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a56bb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction\n",
    "pred_results=regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "63253d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------------+\n",
      "|Independent Features|Salary|       prediction|\n",
      "+--------------------+------+-----------------+\n",
      "|          [24.0,3.0]| 20000|18612.05915813422|\n",
      "+--------------------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3e995dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1387.9408418657804, 1926379.7805190913)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError,pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e795a979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
